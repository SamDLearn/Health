{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# The Nurse Assignment Problem\n\nThis notebook is an example of how **Decision Optimization** can help to prescribe decisions for a complex constrained problem.\n\nWhen you finish this tutorial, you'll have a foundational knowledge of _Prescriptive Analytics_.\n\n>This notebook requires the Commercial Edition of CPLEX engines, which is included in the Default Python 3.6 XS + DO in Watson Studio.\n\n\nTable of contents:\n\n-  [Describe the business problem](#Describe-the-business-problem)\n*  [How decision optimization (prescriptive analytics) can help](#How--decision-optimization-can-help)\n*  [Use decision optimization](#Use-decision-optimization)\n    *  [Step 1: Download the library](#Step-1:-Download-the-library)\n    *  [Step 2: Set up the engines](#Step-2:-Set-up-the-prescriptive-engine)\n    -  [Step 3: Model the data](#Step-3:-Model-the-data)\n    *  [Step 4: Prepare the data](#Step-4:-Prepare-the-data)\n    -  [Step 5: Set up the prescriptive model](#Step-5:-Set-up-the-prescriptive-model)\n        * [Define the decision variables](#Define-the-decision-variables)\n        * [Express the business constraints](#Express-the-business-constraints)\n        * [Express the objective](#Express-the-objective)\n        * [Solve with the Decision Optimization solve service](#Solve-with-the-Decision-Optimization-solve-service)\n    *  [Step 6: Investigate the solution and run an example analysis](#Step-6:-Investigate-the-solution-and-then-run-an-example-analysis)\n*  [Summary](#Summary)\n\n****"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Describe the business problem\n\nThis notebook describes how to use CPLEX Modeling for Python together with *pandas* to\nmanage the assignment of nurses to shifts in a hospital.\n\nNurses must be assigned to hospital shifts in accordance with various skill and staffing constraints.\n\nThe goal of the model is to find an efficient balance between the different objectives:\n\n* Minimize the overall cost of the plan and\n* Assign shifts as fairly as possible.\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## How  decision optimization can help\n\n* Prescriptive analytics (decision optimization) technology recommends actions that are based on desired outcomes.  It takes into account specific scenarios, resources, and knowledge of past and current events. With this insight, your organization can make better decisions and have greater control of business outcomes.  \n\n* Prescriptive analytics is the next step on the path to insight-based actions. It creates value through synergy with predictive analytics, which analyzes data to predict future outcomes.  \n\n* Prescriptive analytics takes that insight to the next level by suggesting the optimal way to handle that future situation. Organizations that can act fast in dynamic conditions and make superior decisions in uncertain environments gain a strong competitive advantage.  \n<br/>\n\n<u>With prescriptive analytics, you can:</u> \n\n* Automate the complex decisions and trade-offs to better manage your limited resources.\n* Take advantage of a future opportunity or mitigate a future risk.\n* Proactively update recommendations based on changing events.\n* Meet operational goals, increase customer loyalty, prevent threats and fraud, and optimize business processes."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Use decision optimization"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step 1: Import the docplex package \n\nThis package is presintalled on Watson Studio."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import sys\nimport docplex.mp\n",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step 2: Model the data\n\nThe input data consists of several tables:\n\n* The Departments table lists all departments in the scope of the assignment.\n* The Skills table list all skills.\n* The Shifts table lists all shifts to be staffed. A shift contains a department, a day in the week, plus the start and end times.\n* The Nurses table lists all nurses, identified by their names.\n* The NurseSkills table gives the skills of each nurse.\n* The SkillRequirements table lists the minimum number of persons required for a given department and skill.\n* The NurseVacations table lists days off for each nurse.\n* The NurseAssociations table lists pairs of nurses who wish to work together.\n* The NurseIncompatibilities table lists pairs of nurses who do not want to work together."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Loading data from Excel with pandas\n\nWe load the data from an Excel file using *pandas*.\nEach sheet is read into a separate *pandas* DataFrame."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# This notebook requires pandas to work\nfrom io import StringIO\nimport json\nimport pandas as pd\nfrom pandas import DataFrame\n\n# Make sure that xlrd package, which is a pandas optional dependency, is installed\n# This package is required for Excel I/O\ntry:\n    import xlrd\nexcept:\n    if hasattr(sys, 'real_prefix'):\n        #we are in a virtual env.\n        !pip install xlrd \n    else:\n        !pip install --user xlrd",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n# Use pandas to read the file, one tab for each table.\nnurse_xls_file = pd.ExcelFile(\"https://dataplatform.cloud.ibm.com/data/exchange-api/v1/entries/2505b070a22403aac9f84884d315219e/data?accessKey=38990e0e17b15be6df05d384006a1d06\")\ndf_skills = nurse_xls_file.parse('Skills')\ndf_depts  = nurse_xls_file.parse('Departments')\ndf_shifts = nurse_xls_file.parse('Shifts')\n# Rename df_shifts index\ndf_shifts.index.name = 'shiftId'\n\n# Index is column 0: name\ndf_nurses = nurse_xls_file.parse('Nurses', header=0, index_col=0)\ndf_nurse_skilles = nurse_xls_file.parse('NurseSkills')\ndf_vacations = nurse_xls_file.parse('NurseVacations')\ndf_associations = nurse_xls_file.parse('NurseAssociations')\ndf_incompatibilities = nurse_xls_file.parse('NurseIncompatibilities')\n\n# Display the nurses dataframe\nprint(\"#nurses = {}\".format(len(df_nurses)))\nprint(\"#shifts = {}\".format(len(df_shifts)))\nprint(\"#vacations = {}\".format(len(df_vacations)))",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "HTTPError",
                    "evalue": "HTTP Error 500: Internal Server Error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-3-37a566fd387f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use pandas to read the file, one tab for each table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnurse_xls_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://dataplatform.cloud.ibm.com/data/exchange-api/v1/entries/2505b070a22403aac9f84884d315219e/data?accessKey=38990e0e17b15be6df05d384006a1d06\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnurse_xls_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Skills'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_depts\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnurse_xls_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Departments'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# can't pass to get_filepath_or_buffer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mfilepath_or_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             filepath_or_buffer, _, _, _ = get_filepath_or_buffer(\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 500: Internal Server Error"
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In addition, we introduce some extra global data:\n\n* The maximum work time for each nurse.\n* The maximum and minimum number of shifts worked by a nurse in a week."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# maximum work time (in hours)\nmax_work_time = 40\n\n# maximum number of shifts worked in a week.\nmax_nb_shifts = 5",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Shifts are stored in a separate DataFrame."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_shifts",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step 3: Prepare the data\n\nWe need to precompute additional data for shifts. \nFor each shift, we need the start time and end time expressed in hours, counting from the beginning of the week: Monday 8am is converted to 8, Tuesday 8am is converted to 24+8 = 32, and so on.\n\n#### Sub-step #1\nWe start by adding an extra column `dow` (day of week) which converts the string \"day\" into an integer in 0..6 (Monday is 0, Sunday is 6)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "days = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\nday_of_weeks = dict(zip(days, range(7)))\n\n# utility to convert a day string e.g. \"Monday\" to an integer in 0..6\ndef day_to_day_of_week(day):\n    return day_of_weeks[day.strip().lower()]\n\n# for each day name, we normalize it by stripping whitespace and converting it to lowercase\n# \" Monday\" -> \"monday\"\ndf_shifts[\"dow\"] = df_shifts.day.apply(day_to_day_of_week)\ndf_shifts",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Sub-step #2 : Compute the absolute start time of each shift.\n\nComputing the start time in the week is easy: just add `24*dow` to column `start_time`. The result is stored in a new column `wstart`."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_shifts[\"wstart\"] = df_shifts.start_time + 24 * df_shifts.dow",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Sub-Step #3 : Compute the absolute end time of each shift.\n\nComputing the absolute end time is a little more complicated as certain shifts span across midnight. For example, Shift #3 starts on Monday at 18:00 and ends Tuesday at 2:00 AM. The absolute end time of Shift #3 is 26, not 2.\nThe general rule for computing absolute end time is:\n\n`abs_end_time = end_time + 24 * dow + (start_time>= end_time ? 24 : 0)`\n\nAgain, we use *pandas* to add a new calculated column `wend`. This is done by using the *pandas* `apply` method with an anonymous `lambda` function over rows. The `raw=True` parameter prevents the creation of a *pandas* Series for each row, which improves the performance significantly on large data sets."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# an auxiliary function to calculate absolute end time of a shift\ndef calculate_absolute_endtime(start, end, dow):\n    return 24*dow + end + (24 if start>=end else 0)\n\n# store the results in a new column\ndf_shifts[\"wend\"] = df_shifts.apply(lambda row: calculate_absolute_endtime(\n        row.start_time, row.end_time, row.dow), axis=1, raw=True)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Sub-step #4 : Compute the duration of each shift.\n\nComputing the duration of each shift is now a straightforward difference of columns. The result is stored in column `duration`."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_shifts[\"duration\"] = df_shifts.wend - df_shifts.wstart",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Sub-step #5 : Compute the minimum demand for each shift.\n\nMinimum demand is the product of duration (in hours) by the minimum required number of nurses. Thus, in number of \nnurse-hours, this demand is stored in another new column `min_demand`.\n\nFinally, we display the updated shifts DataFrame with all calculated columns."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# also compute minimum demand in nurse-hours\ndf_shifts[\"min_demand\"] = df_shifts.min_req * df_shifts.duration\n\n# finally check the modified shifts dataframe\ndf_shifts",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step 4: Set up the prescriptive model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from docplex.mp.environment import Environment\nenv = Environment()\nenv.print_information()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Create the DOcplex model\nThe model contains all the business constraints and defines the objective.\n\nWe now use CPLEX Modeling for Python to build a Mixed Integer Programming (MIP) model for this problem."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from docplex.mp.model import Model\nmdl = Model(name=\"nurses\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Define the decision variables\n\nFor each (nurse, shift) pair, we create one binary variable that is equal to 1 when the nurse is assigned to the shift.\n\nWe use the `binary_var_matrix` method of class `Model`, as each binary variable is indexed by _two_ objects: one nurse and one shift."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# first global collections to iterate upon\nall_nurses = df_nurses.index.values\nall_shifts = df_shifts.index.values\n\n# the assignment variables.\nassigned = mdl.binary_var_matrix(keys1=all_nurses, keys2=all_shifts, name=\"assign_%s_%s\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Express the business constraints\n\n#####  Overlapping shifts\n\nSome shifts overlap in time, and thus cannot be assigned to the same nurse.\nTo check whether two shifts overlap in time, we start by ordering all shifts with respect to their *wstart* and *duration* properties. Then, for each shift, we iterate over the subsequent shifts in this ordered list to easily compute the subset of overlapping shifts.\n\nWe use *pandas* operations to implement this algorithm. But first, we organize all decision variables in a DataFrame.\n\nFor convenience, we also organize the decision variables in a pivot table with *nurses* as row index and *shifts* as columns. The *pandas* *unstack* operation does this."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Organize decision variables in a DataFrame\ndf_assigned = DataFrame({'assigned': assigned})\ndf_assigned.index.names=['all_nurses', 'all_shifts']\n\n# Re-organize the Data Frame as a pivot table with nurses as row index and shifts as columns:\ndf_assigned_pivot = df_assigned.unstack(level='all_shifts')\n\n# Create a pivot using nurses and shifts index as dimensions\ndf_assigned_pivot = df_assigned.reset_index().pivot(index='all_nurses', columns='all_shifts', values='assigned')\n\n# Display first rows of the pivot table\ndf_assigned_pivot.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We create a DataFrame representing a list of shifts sorted by *\"wstart\"* and *\"duration\"*.\nThis sorted list will be used to easily detect overlapping shifts.\n\nNote that indices are reset after sorting so that the DataFrame can be indexed with respect to\nthe index in the sorted list and not the original unsorted list. This is the purpose of the *reset_index()*\noperation which also adds a new column named *\"shiftId\"* with the original index."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create a Data Frame representing a list of shifts sorted by wstart and duration.\n# One keeps only the three relevant columns: 'shiftId', 'wstart' and 'wend' in the resulting Data Frame \ndf_sorted_shifts = df_shifts.sort_values(['wstart','duration']).reset_index()[['shiftId', 'wstart', 'wend']]\n\n# Display the first rows of the newly created Data Frame\ndf_sorted_shifts.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Next, we state that for any pair of shifts that overlap in time, a nurse can be assigned to only one of the two."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "number_of_incompatible_shift_constraints = 0\nfor shift in df_sorted_shifts.itertuples():\n    # Iterate over following shifts\n    # 'shift[0]' contains the index of the current shift in the df_sorted_shifts Data Frame\n    for shift_2 in df_sorted_shifts.iloc[shift[0] + 1:].itertuples():\n        if (shift_2.wstart < shift.wend):\n            # Iterate over all nurses to force incompatible assignment for the current pair of overlapping shifts\n            for nurse_assignments in df_assigned_pivot[[shift.shiftId, shift_2.shiftId]].itertuples():                \n                # this is actually a logical OR\n                mdl.add_constraint(nurse_assignments[1] + nurse_assignments[2] <= 1)\n                number_of_incompatible_shift_constraints += 1\n        else:\n            # No need to test overlap with following shifts\n            break\nprint(\"#incompatible shift constraints: {}\".format(number_of_incompatible_shift_constraints))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Vacations\n\nWhen the nurse is on vacation, he cannot be assigned to any shift starting that day.\n\nWe use the *pandas* *merge* operation to create a join between the *\"df_vacations\"*, *\"df_shifts\"*, and *\"df_assigned\"* DataFrames. Each row of the resulting DataFrame contains the assignment decision variable corresponding to the matching (nurse, shift) pair."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Add 'day of week' column to vacations Data Frame\ndf_vacations['dow'] = df_vacations.day.apply(day_to_day_of_week)\n\n# Join 'df_vacations', 'df_shifts' and 'df_assigned' Data Frames to create the list of 'forbidden' assigments.\n# The 'reset_index()' function is invoked to move 'shiftId' index as a column in 'df_shifts' Data Frame, and\n# to move the index pair ('all_nurses', 'all_shifts') as columns in 'df_assigned' Data Frame.\n# 'reset_index()' is invoked so that a join can be performed between Data Frame, based on column names.\ndf_assigned_reindexed = df_assigned.reset_index()\ndf_vacation_forbidden_assignments = df_vacations.merge(df_shifts.reset_index()[['dow', 'shiftId']]).merge(\n    df_assigned_reindexed, left_on=['nurse', 'shiftId'], right_on=['all_nurses', 'all_shifts'])\n\n# Here are the first few rows of the resulting Data Frames joins\ndf_vacation_forbidden_assignments.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for forbidden_assignment in df_vacation_forbidden_assignments.itertuples():\n    # to forbid an assignment just set the variable to zero.\n    mdl.add_constraint(forbidden_assignment.assigned == 0)\nprint(\"# vacation forbids: {} assignments\".format(len(df_vacation_forbidden_assignments)))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Associations\n\nSome pairs of nurses get along particularly well, so we wish to assign them together as a team. In other words, for every such couple and for each shift, both assignment variables should always be equal.\nEither both nurses work the shift, or both do not.\n\nIn the same way we modeled *vacations*, we use the *pandas* merge operation to create a DataFrame for which each row contains the pair of nurse-shift assignment decision variables matching each association."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Join 'df_assignment' Data Frame twice, based on associations to get corresponding decision variables pairs for all shifts\n# The 'suffixes' parameter in the second merge indicates our preference for updating the name of columns that occur both\n# in the first and second argument Data Frames (in our case, these columns are 'all_nurses' and 'assigned').\ndf_preferred_assign = df_associations.merge(\n    df_assigned_reindexed, left_on='nurse1', right_on='all_nurses').merge(\n    df_assigned_reindexed, left_on=['nurse2', 'all_shifts'], right_on=['all_nurses', 'all_shifts'], suffixes=('_1','_2'))\n\n# Here are the first few rows of the resulting Data Frames joins\ndf_preferred_assign.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The associations constraint can now easily be formulated by iterating on the rows of the *\"df_preferred_assign\"* DataFrame."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for preferred_assign in df_preferred_assign.itertuples():\n    mdl.add_constraint(preferred_assign.assigned_1 == preferred_assign.assigned_2)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Incompatibilities\n\nSimilarly, certain pairs of nurses do not get along well, and we want to avoid having them together on a shift.\nIn other terms, for each shift, both nurses of an incompatible pair cannot be assigned together to the sift. Again, we state a logical OR between the two assignments: at most one nurse from the pair can work the shift.\n\nWe first create a DataFrame whose rows contain pairs of invalid assignment decision variables, using the same *pandas* `merge` operations as in the previous step."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Join assignment Data Frame twice, based on incompatibilities Data Frame to get corresponding decision variables pairs\n#  for all shifts\ndf_incompatible_assign = df_incompatibilities.merge(\n    df_assigned_reindexed, left_on='nurse1', right_on='all_nurses').merge(\n    df_assigned_reindexed, left_on=['nurse2', 'all_shifts'], right_on=['all_nurses', 'all_shifts'], suffixes=('_1','_2'))\n\n# Here are the first few rows of the resulting Data Frames joins\ndf_incompatible_assign.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The incompatibilities constraint can now easily be formulated, by iterating on the rows of the *\"df_incompatible_assign\"* DataFrame."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for incompatible_assign in df_incompatible_assign.itertuples():\n    mdl.add_constraint(incompatible_assign.assigned_1 + incompatible_assign.assigned_2 <= 1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Constraints on work time\n\nRegulations force constraints on the total work time over a week;\nand we compute this total work time in a new variable. We store the variable in an extra column in the nurse DataFrame.\n\nThe variable is declared as _continuous_ though it contains only integer values. This is done to avoid adding unnecessary integer variables for the _branch and bound_ algorithm. \nThese variables are not true decision variables; they are used to express work constraints.\n\nFrom a *pandas* perspective, we apply a function over the rows of the nurse DataFrame to create this variable and store it into a new column of the DataFrame."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# auxiliary function to create worktime variable from a row\ndef make_var(row, varname_fmt):\n    return mdl.continuous_var(name=varname_fmt % row.name, lb=0)\n\n# apply the function over nurse rows and store result in a new column\ndf_nurses[\"worktime\"] = df_nurses.apply(lambda r: make_var(r, \"worktime_%s\"), axis=1)\n\n# display nurse dataframe\ndf_nurses",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "###### Define total work time\n\nWork time variables must be constrained to be equal to the sum of hours actually worked.\n\nWe use the *pandas* *groupby* operation to collect all assignment decision variables for each nurse in a separate series. Then, we iterate over nurses to post a constraint calculating the actual worktime for each nurse as the dot product of the series of nurse-shift assignments with the series of shift durations."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use pandas' groupby operation to enforce constraint calculating worktime for each nurse as the sum of all assigned\n#  shifts times the duration of each shift\nfor nurse, nurse_assignments in df_assigned.groupby(level='all_nurses'):\n    mdl.add_constraint(df_nurses.worktime[nurse] == mdl.dot(nurse_assignments.assigned, df_shifts.duration))\n                       \n# print model information and check we now have 32 extra continuous variables\nmdl.print_information()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "###### Maximum work time\n\nFor each nurse, we add a constraint to enforce the maximum work time for a week.\nAgain we use the `apply` method, this time with an anonymous lambda function."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# we use pandas' apply() method to set an upper bound on all worktime variables.\ndef set_max_work_time(v):\n    v.ub = max_work_time\n    # Optionally: return a string for fancy display of the constraint in the Output cell\n    return str(v) + ' <= ' + str(v.ub)\n\ndf_nurses[\"worktime\"].apply(convert_dtype=False, func=set_max_work_time)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Minimum requirement for shifts\n\nEach shift requires a minimum number of nurses. \nFor each shift, the sum over all nurses of assignments to this shift\nmust be greater than the minimum requirement.\n\nThe *pandas* *groupby* operation is invoked to collect all assignment decision variables for each shift in a separate series. Then, we iterate over shifts to post the constraint enforcing the minimum number of nurse assignments for each shift."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use pandas' groupby operation to enforce minimum requirement constraint for each shift\nfor shift, shift_nurses in df_assigned.groupby(level='all_shifts'):\n    mdl.add_constraint(mdl.sum(shift_nurses.assigned) >= df_shifts.min_req[shift])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Express the objective\n\nThe objective mixes different (and contradictory) KPIs. \n\nThe first KPI is the total salary cost, computed as the sum of work times over all nurses, weighted by pay rate.\n\nWe compute this KPI as an expression from the variables we previously defined by using the panda summation over the DOcplex objects."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# again leverage pandas to create a series of expressions: costs of each nurse\ntotal_salary_series = df_nurses.worktime * df_nurses.pay_rate\n\n# compute global salary cost using pandas sum()\n# Note that the result is a DOcplex expression: DOcplex if fully compatible with pandas\ntotal_salary_cost = total_salary_series.sum()\nmdl.add_kpi(total_salary_cost, \"Total salary cost\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Minimizing salary cost\n\nIn a preliminary version of the model, we minimize the total salary cost. This is accomplished\nusing the `Model.minimize()` method."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mdl.minimize(total_salary_cost)\nmdl.print_information()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Solve with the Decision Optimization solve service\n\nNow we have everything we need to solve the model, using `Model.solve()`. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Set Cplex mipgap to 1e-5 to enforce precision to be of the order of a unit (objective value magnitude is ~1e+5).\nmdl.parameters.mip.tolerances.mipgap = 1e-5\n\ns = mdl.solve(log_output=True)\nassert s, \"solve failed\"\nmdl.report()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step 5: Investigate the solution and then run an example analysis\n\nWe take advantage of *pandas* to analyze the results. First we store the solution values of the assignment variables into a new *pandas* Series.\n\nCalling `solution_value` on a DOcplex variable returns its value in the solution (provided the model has been successfully solved)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create a pandas Series containing actual shift assignment decision variables value\ns_assigned = df_assigned.assigned.apply(lambda v: v.solution_value)\n\n# Create a pivot table by (nurses, shifts), using pandas' \"unstack\" method to transform the 'all_shifts' row index\n#  into columns\ndf_res = s_assigned.unstack(level='all_shifts')\n\n# Display the first few rows of the resulting pivot table\ndf_res.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Analyzing how worktime is distributed\n\nLet's analyze how worktime is distributed among nurses. \n\nFirst, we compute the global average work time as the total minimum requirement in hours, divided by number of nurses."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "s_demand  = df_shifts.min_req * df_shifts.duration\ntotal_demand = s_demand.sum()\navg_worktime = total_demand / float(len(all_nurses))\nprint(\"* theoretical average work time is {0:g} h\".format(avg_worktime))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's analyze the series of deviations to the average, stored in a *pandas* Series."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# a pandas series of worktimes solution values\ns_worktime = df_nurses.worktime.apply(lambda v: v.solution_value)\n\n# returns a new series computed as deviation from average\ns_to_mean = s_worktime - avg_worktime\n\n# take the absolute value\ns_abs_to_mean = s_to_mean.apply(abs)\n\n\ntotal_to_mean = s_abs_to_mean.sum()\nprint(\"* the sum of absolute deviations from mean is {}\".format(total_to_mean))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "To see how work time is distributed among nurses, print a histogram of work time values.\nNote that, as all time data are integers, work times in the solution can take only integer values."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import matplotlib.pyplot as plt\n%matplotlib inline\n\n# we can also plot as a histogram the distribution of worktimes\ns_worktime.plot.hist(color='LightBlue')\nplt.xlabel(\"worktime\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### How shifts are distributed\n\nLet's now analyze the solution from the _number of shifts_ perspective.\nHow many shifts does each nurse work? Are these shifts fairly distributed amongst nurses?\n\nWe compute a new column in our result DataFrame for the number of shifts worked,\nby summing rows (the *\"axis=1\"* argument in the *sum()* call indicates to *pandas* that each sum is performed by row instead of column):"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# a pandas series of #shifts worked\ndf_worked = df_res[all_shifts].sum(axis=1)\ndf_res[\"worked\"] = df_worked\n\ndf_worked.plot.hist(color=\"gold\", xlim=(0,10))\nplt.ylabel(\"#shifts worked\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We see that one nurse works significantly fewer shifts than others do. What is the average number of shifts worked by a nurse? This is equal to the total demand divided by the number of nurses.\n\nOf course, this yields a fractional number of shifts that is not practical, but nonetheless will help us quantify\nthe _fairness_ in shift distribution."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "avg_worked = df_shifts[\"min_req\"].sum() / float(len(all_nurses))\nprint(\"-- expected avg #shifts worked is {}\".format(avg_worked))\n\nworked_to_avg = df_res[\"worked\"] - avg_worked\ntotal_to_mean = worked_to_avg.apply(abs).sum()\nprint(\"-- total absolute deviation to mean #shifts is {}\".format(total_to_mean))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Introducing a fairness goal\n\nAs the above diagram suggests, the distribution of shifts could be improved.\nWe implement this by adding one extra objective, _fairness_, which balances\nthe shifts assigned over nurses.\n\nNote that we can edit the model, that  is add (or remove) constraints, even after it has been solved. \n\n### Step #1 : Introduce three new variables per nurse to model the \nnumber of shifts worked and positive and negative deviations to the average."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# add two extra variables per nurse: deviations above and below average\ndf_nurses[\"worked\"]      = df_nurses.apply(lambda r: make_var(r, \"worked%s\"), axis=1)\ndf_nurses[\"overworked\"]  = df_nurses.apply(lambda r: make_var(r, \"overw_%s\"), axis=1)\ndf_nurses[\"underworked\"] = df_nurses.apply(lambda r: make_var(r, \"underw_%s\"), axis=1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step #2 : Post the constraint that links these variables together."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use the pandas groupby operation to enforce the constraint calculating number of worked shifts for each nurse\nfor nurse, nurse_assignments in df_assigned.groupby(level='all_nurses'):\n    # nb of worked shifts is sum of assigned shifts\n    mdl.add_constraint(df_nurses.worked[nurse] == mdl.sum(nurse_assignments.assigned))\n\nfor nurse in df_nurses.itertuples():\n    # nb worked is average + over - under\n    mdl.add_constraint(nurse.worked == avg_worked + nurse.overworked - nurse.underworked)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Step #3 : Define KPIs to measure the result after solve."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# finally, define kpis for over and under average quantities\ntotal_overw = mdl.sum(df_nurses[\"overworked\"])\nmdl.add_kpi(total_overw, \"Total over-worked\")\ntotal_underw = mdl.sum(df_nurses[\"underworked\"])\nmdl.add_kpi(total_underw, \"Total under-worked\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Finally, let's modify the objective by adding the sum of `over_worked and under_worked` to the previous objective.\n\n**Note:** The definitions of `over_worked` and `under_worked` as described above are not sufficient to give them an unambiguous value. However, as all these variables are minimized, CPLEX ensures that these variables take the minimum possible values in the solution."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mdl.minimize(total_salary_cost + total_overw + total_underw)  # incorporate over_worked and under_worked in objective",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Our modified model is ready to solve. \n\nThe `log_output=True` parameter tells CPLEX to print the log on the standard output."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sol2 = mdl.solve(log_output=True)  # solve again and get a new solution\nassert sol2, \"Solve failed\"\nmdl.report()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Analyzing new results\n\nLet's recompute the new total deviation from average on this new solution."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create a pandas Series containing actual shift assignment decision variables value\ns_assigned2 = df_assigned.assigned.apply(lambda v: v.solution_value)\n\n# Create a pivot table by (nurses, shifts), using pandas' \"unstack\" method to transform the 'all_shifts' row index\n#  into columns\ndf_res2 = s_assigned2.unstack(level='all_shifts')\n\n# Add a new column to the pivot table containing the #shifts worked by summing over each row\ndf_res2[\"worked\"] = df_res2[all_shifts].sum(axis=1)\n\n# total absolute deviation from average is directly read on expressions\nnew_total_to_mean = total_overw.solution_value + total_underw.solution_value\nprint(\"-- total absolute deviation to mean #shifts is now {0} down from {1}\".format(new_total_to_mean, total_to_mean))\n\n# Display the first few rows of the result Data Frame\ndf_res2.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's print the new histogram of shifts worked."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_res2[\"worked\"].plot(kind=\"hist\", color=\"gold\", xlim=(3,8))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### The breakdown of shifts over nurses is much closer to the average than it was in the previous version."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### But what would be the minimal fairness level?\n\nBut what is the absolute minimum for the deviation to the ideal average number of shifts?\nCPLEX can tell us: simply minimize only the the total deviation from average, ignoring the salary cost.\nOf course this is unrealistic, but it will help us quantify how far our fairness result is to the\nabsolute optimal fairness.\n\nWe modify the objective and solve for the third time."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mdl.minimize(total_overw + total_underw)\nassert mdl.solve(), \"solve failed\"\nmdl.report()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In the fairness-optimal solution, we have zero under-average shifts and 4 over-average.\nSalary cost is now higher than the previous value of 28884 but this was expected as salary cost was not part of the objective.\n\nTo summarize, the absolute minimum for this measure of fairness is 4, and we have found a balance with fairness=7.\n\nFinally, we display the histogram for this optimal-fairness solution."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create a pandas Series containing actual shift assignment decision variables value\ns_assigned_fair = df_assigned.assigned.apply(lambda v: v.solution_value)\n\n# Create a pivot table by (nurses, shifts), using pandas' \"unstack\" method to transform the 'all_shifts' row index\n#  into columns\ndf_res_fair = s_assigned_fair.unstack(level='all_shifts')\n\n# Add a new column to the pivot table containing the #shifts worked by summing over each row\ndf_res_fair[\"solution_value_fair\"] = df_res_fair[all_shifts].sum(axis=1)\ndf_res_fair[\"worked\"] = df_res_fair[all_shifts].sum(axis=1)\ndf_res_fair[\"worked\"].plot.hist(color=\"plum\", xlim=(3,8))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In the above figure, all nurses but one are assigned the average of 7 shifts, which is what we expected.\n\n## Summary\n\nWe have learned how to set up, formulate and solve an optimization model using Decision Optimization in Watson Studio."
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}