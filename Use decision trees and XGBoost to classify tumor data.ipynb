{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Use decision trees and XGBoost to classify tumor data\n \nThis notebook explains how to use machine learning to classify tumor data. You develop your solution in three parts as follows:\n\n* An intuitive introduction to supervised learning concepts\n\n* A basic example of a machine learning model. \n\n* A deep dive into model stacking and parameter tuning, both of which are used in practice to significantly improve predictive accuracy\n\nSome guidelines for reading this notebook:\n\n* If you have no experience with machine learning: follow the entire notebook for a comprehensive walkthrough.\n\n* If you would like to read about decision trees, go to [section 2](#bullet-8).\n\n* If you would like to read about using XGBoost in practice, go to [section 3](#bullet-13).\n\nSome familiarity with Python is recommended"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Table of contents\n1.0 [Introduction to supervised learning](#bullet-1)<br/>\n2.0 [Basic model: decision trees](#bullet-8)<br/> \n3.0 [Ensemble model: gradient boosting](#bullet-13)  \n4.0 [XGBoost: parameter tuning](#bullet-17)\n    "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 1.0 Introduction to supervised learning<a class=\"anchor\" id=\"bullet-1\"></a>\n\n1.1 [What is machine learning?](#bullet-2)<br/>\n    1.2 [Defining the task](#bullet-3)<br/>\n    1.3 [How does an algorithm learn?](#bullet-4)<br/>\n    1.4 [Data preview](#bullet-5)<br/>\n    1.5 [Pre-processing](#bullet-6)<br/>\n    1.7 [Create train and test sets](#bullet-7)<br/>\n\n### 1.1 What is machine learning?<a class=\"anchor\" id=\"bullet-2\"></a>\n\nYou use an algorithm (code) to create a model (mathematical function). The algorithm tries to find patterns in a sample of **training data** that can be used on future data. The model itself is a summary of these patterns, distilled into mathematical relationships between variables. The algorithm depends on **hyperparameters** that control how it looks for these patterns (that is, how it learns). \n\nThe difficult part is to find parameters that balance accuracy and precision of the model with its ability to generalize.\n\n**Note:**\n* Accuracy and precision of predictions depend on the complexity of the model\n* Ability to generalize depends on how conservative the model is (how quickly it concludes that something is a pattern)\n\nThis balancing act is called the **bias-variance tradeoff**, because a very complex model will represent all the specific nuances of its training data, therefore failing to generalize. In the opposite case, a simple model does not pick up enough of a pattern to ensure accuracy and precision of future predictions. \n\nYour job* is to pick an efficient algorithm and suitable model for our data set and learning task, and fine-tune its parameters to produce a balanced model.\n\n**apart from the arduous task of data pre-processing, which this notebook skims over for the sake of brevity (and sanity!)*\n\n\n### 1.2 Defining the task<a class=\"anchor\" id=\"bullet-3\"></a>\n\nThe goal is to use an augmented version <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\" target=\"_blank\" rel=\"noopener noreferrer\">this data set</a> to develop a predictive model which classifies breast tumors as malignant or benign depending on measurements of the tumor cells. The <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/c173693bf48aeb22e41bbe2b41d79c1f\" target=\"_blank\" rel=\"noopener noreferrer\">augmented data set</a> includes statistical analysis values, such as the mean, for some of the measurements. In machine learning terms, the **target** (what you want to predict) is the diagnosis, and the **features** are the measurements. The data sample contains **observations** of tumor cases described by these two components.\n\n\n### 1.3 How does an algorithm learn?<a class=\"anchor\" id=\"bullet-4\"></a>\n\n1. Initialize the algorithm to produce a default model, and give it the feature data. The algorithm makes its first set of predictions (close to a random guess).\n2. The algorithm measures the error between its previous prediction and the true value of the targets. \n3. The algorithm adjusts its model-building to make the error smaller. Each adjustment represents a part of the pattern it is learning, and is used to process future data.\n4. It continues to predict, calculate error, and adjust.\n\nRemember that the algorithm is only using a training sample to perform step 3. If the algorithm makes an adjustment so that its predictions are *exactly* the targets, then it is **overfitted** to the training data. This means that it has lost the ability to generalize to new (unseen) observations, because it picked up a pattern that only applies to the data points it learned from.\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Load data\nTo load the data:\n1. Go to the <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/c173693bf48aeb22e41bbe2b41d79c1f\" target=\"_blank\" rel=\"noopener noreferrer\">Breast Cancer Wisconsin (Diagnostic) Data Set</a> on the Watson Studio Community. \n1. Click the download icon and save the data set to your computer.  \n1. Load the `BreastCancerWisconsonDataSet.csv` file into your notebook. Click the **Data** icon on the notebook action bar. Drop the file into the box or browse to select the file. The file is loaded to your object storage and appears in the Data Assets section of the project. For more information, see <a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/load-and-access-data.html\" target=\"_blank\" rel=\"noopener noreferrer\">Load and access data</a>.\n1. To load the data from the `BreastCancerWisconsonDataSet.csv` file into a pandas DataFrame, click in the next code cell and select **Insert to code > Insert pandas DataFrame** under the file name.\n1. Make sure the object is named `df` instead of `df_data_1` in the last two lines of the **Insert to Code** block and then run the cell."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# click in this cell\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_ec95c633090441099a8c5ecc71550224 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='hrdnVScSXm_Ub_Fy8XiSU6EDld79-0fIfEGzX2zEmvjQ',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_ec95c633090441099a8c5ecc71550224.get_object(Bucket='health-donotdelete-pr-retyx20ewuwekd',Key='BreastCancerwisconsin_data.csv.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# If you are reading an Excel file into a pandas DataFrame, replace `read_csv` by `read_excel` in the next statement.\ndf = pd.read_csv(body)\ndf.head()\n",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 1,
                    "data": {
                        "text/plain": "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 33 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "### 1.4 Data preview<a class=\"anchor\" id=\"bullet-5\"></a>\n\nIn the following data table, each row corresponds to one observation of a tumor. The columns contain the measurement features, with one column containing the diagnosis target."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from IPython.display import display\nprint (df.shape)\ndisplay(df[200:210])",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(569, 33)\n",
                    "name": "stdout"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "          id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n200   877501         B       12.230         19.56           78.54      461.0   \n201   877989         M       17.540         19.32          115.10      951.6   \n202   878796         M       23.290         26.67          158.90     1685.0   \n203    87880         M       13.810         23.75           91.56      597.8   \n204    87930         B       12.470         18.60           81.09      481.9   \n205   879523         M       15.120         16.68           98.78      716.6   \n206   879804         B        9.876         17.27           62.92      295.4   \n207   879830         M       17.010         20.26          109.70      904.3   \n208  8810158         B       13.110         22.54           87.02      529.4   \n209  8810436         B       15.270         12.91           98.17      725.5   \n\n     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n200          0.09586           0.08087         0.04187              0.04107   \n201          0.08968           0.11980         0.10360              0.07488   \n202          0.11410           0.20840         0.35230              0.16200   \n203          0.13230           0.17680         0.15580              0.09176   \n204          0.09965           0.10580         0.08005              0.03821   \n205          0.08876           0.09588         0.07550              0.04079   \n206          0.10890           0.07232         0.01756              0.01952   \n207          0.08772           0.07304         0.06950              0.05390   \n208          0.10020           0.14830         0.08705              0.05102   \n209          0.08182           0.06230         0.05892              0.03157   \n\n     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n200  ...          28.36            92.15       638.4            0.1429   \n201  ...          25.84           139.50      1239.0            0.1381   \n202  ...          32.68           177.00      1986.0            0.1536   \n203  ...          41.85           128.50      1153.0            0.2226   \n204  ...          24.64            96.05       677.9            0.1426   \n205  ...          20.24           117.70       989.5            0.1491   \n206  ...          23.22            67.08       331.6            0.1415   \n207  ...          25.05           130.00      1210.0            0.1111   \n208  ...          29.16            99.48       639.3            0.1349   \n209  ...          15.92           113.70       932.7            0.1222   \n\n     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n200             0.2042          0.13770               0.10800          0.2668   \n201             0.3420          0.35080               0.19390          0.2928   \n202             0.4167          0.78920               0.27330          0.3198   \n203             0.5209          0.46460               0.20130          0.4432   \n204             0.2378          0.26710               0.10150          0.3014   \n205             0.3331          0.33270               0.12520          0.3415   \n206             0.1247          0.06213               0.05588          0.2989   \n207             0.1486          0.19320               0.10960          0.3275   \n208             0.4402          0.31620               0.11260          0.4128   \n209             0.2186          0.29620               0.10350          0.2320   \n\n     fractal_dimension_worst  Unnamed: 32  \n200                  0.08174          NaN  \n201                  0.07867          NaN  \n202                  0.08762          NaN  \n203                  0.10860          NaN  \n204                  0.08750          NaN  \n205                  0.09740          NaN  \n206                  0.07380          NaN  \n207                  0.06469          NaN  \n208                  0.10760          NaN  \n209                  0.07474          NaN  \n\n[10 rows x 33 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>200</th>\n      <td>877501</td>\n      <td>B</td>\n      <td>12.230</td>\n      <td>19.56</td>\n      <td>78.54</td>\n      <td>461.0</td>\n      <td>0.09586</td>\n      <td>0.08087</td>\n      <td>0.04187</td>\n      <td>0.04107</td>\n      <td>...</td>\n      <td>28.36</td>\n      <td>92.15</td>\n      <td>638.4</td>\n      <td>0.1429</td>\n      <td>0.2042</td>\n      <td>0.13770</td>\n      <td>0.10800</td>\n      <td>0.2668</td>\n      <td>0.08174</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>877989</td>\n      <td>M</td>\n      <td>17.540</td>\n      <td>19.32</td>\n      <td>115.10</td>\n      <td>951.6</td>\n      <td>0.08968</td>\n      <td>0.11980</td>\n      <td>0.10360</td>\n      <td>0.07488</td>\n      <td>...</td>\n      <td>25.84</td>\n      <td>139.50</td>\n      <td>1239.0</td>\n      <td>0.1381</td>\n      <td>0.3420</td>\n      <td>0.35080</td>\n      <td>0.19390</td>\n      <td>0.2928</td>\n      <td>0.07867</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>878796</td>\n      <td>M</td>\n      <td>23.290</td>\n      <td>26.67</td>\n      <td>158.90</td>\n      <td>1685.0</td>\n      <td>0.11410</td>\n      <td>0.20840</td>\n      <td>0.35230</td>\n      <td>0.16200</td>\n      <td>...</td>\n      <td>32.68</td>\n      <td>177.00</td>\n      <td>1986.0</td>\n      <td>0.1536</td>\n      <td>0.4167</td>\n      <td>0.78920</td>\n      <td>0.27330</td>\n      <td>0.3198</td>\n      <td>0.08762</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>87880</td>\n      <td>M</td>\n      <td>13.810</td>\n      <td>23.75</td>\n      <td>91.56</td>\n      <td>597.8</td>\n      <td>0.13230</td>\n      <td>0.17680</td>\n      <td>0.15580</td>\n      <td>0.09176</td>\n      <td>...</td>\n      <td>41.85</td>\n      <td>128.50</td>\n      <td>1153.0</td>\n      <td>0.2226</td>\n      <td>0.5209</td>\n      <td>0.46460</td>\n      <td>0.20130</td>\n      <td>0.4432</td>\n      <td>0.10860</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>87930</td>\n      <td>B</td>\n      <td>12.470</td>\n      <td>18.60</td>\n      <td>81.09</td>\n      <td>481.9</td>\n      <td>0.09965</td>\n      <td>0.10580</td>\n      <td>0.08005</td>\n      <td>0.03821</td>\n      <td>...</td>\n      <td>24.64</td>\n      <td>96.05</td>\n      <td>677.9</td>\n      <td>0.1426</td>\n      <td>0.2378</td>\n      <td>0.26710</td>\n      <td>0.10150</td>\n      <td>0.3014</td>\n      <td>0.08750</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>879523</td>\n      <td>M</td>\n      <td>15.120</td>\n      <td>16.68</td>\n      <td>98.78</td>\n      <td>716.6</td>\n      <td>0.08876</td>\n      <td>0.09588</td>\n      <td>0.07550</td>\n      <td>0.04079</td>\n      <td>...</td>\n      <td>20.24</td>\n      <td>117.70</td>\n      <td>989.5</td>\n      <td>0.1491</td>\n      <td>0.3331</td>\n      <td>0.33270</td>\n      <td>0.12520</td>\n      <td>0.3415</td>\n      <td>0.09740</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>879804</td>\n      <td>B</td>\n      <td>9.876</td>\n      <td>17.27</td>\n      <td>62.92</td>\n      <td>295.4</td>\n      <td>0.10890</td>\n      <td>0.07232</td>\n      <td>0.01756</td>\n      <td>0.01952</td>\n      <td>...</td>\n      <td>23.22</td>\n      <td>67.08</td>\n      <td>331.6</td>\n      <td>0.1415</td>\n      <td>0.1247</td>\n      <td>0.06213</td>\n      <td>0.05588</td>\n      <td>0.2989</td>\n      <td>0.07380</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>879830</td>\n      <td>M</td>\n      <td>17.010</td>\n      <td>20.26</td>\n      <td>109.70</td>\n      <td>904.3</td>\n      <td>0.08772</td>\n      <td>0.07304</td>\n      <td>0.06950</td>\n      <td>0.05390</td>\n      <td>...</td>\n      <td>25.05</td>\n      <td>130.00</td>\n      <td>1210.0</td>\n      <td>0.1111</td>\n      <td>0.1486</td>\n      <td>0.19320</td>\n      <td>0.10960</td>\n      <td>0.3275</td>\n      <td>0.06469</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>8810158</td>\n      <td>B</td>\n      <td>13.110</td>\n      <td>22.54</td>\n      <td>87.02</td>\n      <td>529.4</td>\n      <td>0.10020</td>\n      <td>0.14830</td>\n      <td>0.08705</td>\n      <td>0.05102</td>\n      <td>...</td>\n      <td>29.16</td>\n      <td>99.48</td>\n      <td>639.3</td>\n      <td>0.1349</td>\n      <td>0.4402</td>\n      <td>0.31620</td>\n      <td>0.11260</td>\n      <td>0.4128</td>\n      <td>0.10760</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>8810436</td>\n      <td>B</td>\n      <td>15.270</td>\n      <td>12.91</td>\n      <td>98.17</td>\n      <td>725.5</td>\n      <td>0.08182</td>\n      <td>0.06230</td>\n      <td>0.05892</td>\n      <td>0.03157</td>\n      <td>...</td>\n      <td>15.92</td>\n      <td>113.70</td>\n      <td>932.7</td>\n      <td>0.1222</td>\n      <td>0.2186</td>\n      <td>0.29620</td>\n      <td>0.10350</td>\n      <td>0.2320</td>\n      <td>0.07474</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows \u00d7 33 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The data contains features extracted from 569 diagnostic images of breast tumors. The diagnosis column indicates whether the mass was benign (B), or malignant (M). The rest of the columns contain features which are structured as follows:\n\n10 variables describe the cell nuclei of each mass, and for each variable, the mean, standard deviation, and 'worst' (mean of three largest measurements) are calculated. The variables are briefly described on the <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\" target=\"_blank\" rel=\"noopener noreferrer\">original data set page</a>.\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 1.5 Pre-processing<a class=\"anchor\" id=\"bullet-6\"></a>\n\nThe following code cell gently cleans the data and generates a summary that can be used to check for outliers. This data set is well prepared for analysis and does not require further manipulation.\n\nIf you don't have scikit-learn library installed, install it."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from sklearn.preprocessing import LabelEncoder\n\n# checking for missing values\ndf.isnull().any()\n\n# dropping Unnamed column\n#df2 = df.drop(['Unnamed: 32'], axis=1)\n\n# converting diagnosis M/B to numerical\nlenc = LabelEncoder()\nlenc.fit(df['diagnosis'])\ndf['diagnosis'] = lenc.transform(df['diagnosis'])\n\n# overview of data sample\ndf.describe()",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "                 id   diagnosis  radius_mean  texture_mean  perimeter_mean  \\\ncount  5.690000e+02  569.000000   569.000000    569.000000      569.000000   \nmean   3.037183e+07    0.372583    14.127292     19.289649       91.969033   \nstd    1.250206e+08    0.483918     3.524049      4.301036       24.298981   \nmin    8.670000e+03    0.000000     6.981000      9.710000       43.790000   \n25%    8.692180e+05    0.000000    11.700000     16.170000       75.170000   \n50%    9.060240e+05    0.000000    13.370000     18.840000       86.240000   \n75%    8.813129e+06    1.000000    15.780000     21.800000      104.100000   \nmax    9.113205e+08    1.000000    28.110000     39.280000      188.500000   \n\n         area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\ncount   569.000000       569.000000        569.000000      569.000000   \nmean    654.889104         0.096360          0.104341        0.088799   \nstd     351.914129         0.014064          0.052813        0.079720   \nmin     143.500000         0.052630          0.019380        0.000000   \n25%     420.300000         0.086370          0.064920        0.029560   \n50%     551.100000         0.095870          0.092630        0.061540   \n75%     782.700000         0.105300          0.130400        0.130700   \nmax    2501.000000         0.163400          0.345400        0.426800   \n\n       concave points_mean  ...  texture_worst  perimeter_worst   area_worst  \\\ncount           569.000000  ...     569.000000       569.000000   569.000000   \nmean              0.048919  ...      25.677223       107.261213   880.583128   \nstd               0.038803  ...       6.146258        33.602542   569.356993   \nmin               0.000000  ...      12.020000        50.410000   185.200000   \n25%               0.020310  ...      21.080000        84.110000   515.300000   \n50%               0.033500  ...      25.410000        97.660000   686.500000   \n75%               0.074000  ...      29.720000       125.400000  1084.000000   \nmax               0.201200  ...      49.540000       251.200000  4254.000000   \n\n       smoothness_worst  compactness_worst  concavity_worst  \\\ncount        569.000000         569.000000       569.000000   \nmean           0.132369           0.254265         0.272188   \nstd            0.022832           0.157336         0.208624   \nmin            0.071170           0.027290         0.000000   \n25%            0.116600           0.147200         0.114500   \n50%            0.131300           0.211900         0.226700   \n75%            0.146000           0.339100         0.382900   \nmax            0.222600           1.058000         1.252000   \n\n       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\ncount            569.000000      569.000000               569.000000   \nmean               0.114606        0.290076                 0.083946   \nstd                0.065732        0.061867                 0.018061   \nmin                0.000000        0.156500                 0.055040   \n25%                0.064930        0.250400                 0.071460   \n50%                0.099930        0.282200                 0.080040   \n75%                0.161400        0.317900                 0.092080   \nmax                0.291000        0.663800                 0.207500   \n\n       Unnamed: 32  \ncount          0.0  \nmean           NaN  \nstd            NaN  \nmin            NaN  \n25%            NaN  \n50%            NaN  \n75%            NaN  \nmax            NaN  \n\n[8 rows x 33 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.690000e+02</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.037183e+07</td>\n      <td>0.372583</td>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>...</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.250206e+08</td>\n      <td>0.483918</td>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>...</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>8.670000e+03</td>\n      <td>0.000000</td>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.692180e+05</td>\n      <td>0.000000</td>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>...</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>9.060240e+05</td>\n      <td>0.000000</td>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>...</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.813129e+06</td>\n      <td>1.000000</td>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>...</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.113205e+08</td>\n      <td>1.000000</td>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>...</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows \u00d7 33 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 1.6 Create train and test sets<a class=\"anchor\" id=\"bullet-7\"></a>\n\nTake 80% of the total data to train the model, and reserve 20% of it for testing. After the model is built and tuned upon the training set, you can use the testing set to observe how well the model is able to generalize to new 'unseen' data."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from sklearn.model_selection import train_test_split\n\n## X are all the features (columns) that might be useful to the model\n## y is the target (diagnosis column)\nX, y = df.ix[:,'radius_mean':], df[['diagnosis']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: \n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 2.0 Basic model: decision trees<a class=\"anchor\" id=\"bullet-8\"></a>\n\n2.1 [Mimicking human decision-making](#bullet-9)<br/>\n    2.2 [Interpreting a tree graphic](#bullet-10)<br/>\n    2.3 [How a decision tree learns](#bullet-11)<br/>\n    2.3 [Evaluating the model](#bullet-12)<br/>\n\n### 2.1 Mimicking human decision-making<a class=\"anchor\" id=\"bullet-9\"></a>\n\nA decision tree is an algorithm that can be used for machine learning. It is analogous to the game <a href=\"http://www.wikihow.com/Play-20-Questions\" target=\"_blank\" rel=\"noopener noreferrer\">20 questions</a> - each 'question' is a **splitting feature** chosen from the data based on how useful it is for identifying the target. The algorithm decides on these features by optimizing a variety of mathematical functions that quantify prediction error (<a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\" target=\"_blank\" rel=\"noopener noreferrer\">for the curious</a>).\n\nTrees are considered weak learners, because their predictions are usually only slightly better than chance. The following code builds a tree based on the tumor data, and displays the process graphically.\n\nIf you don't have pydotplus library installed, install it."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install pydotplus",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting pydotplus\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz (278kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 286kB 7.9MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pydotplus) (2.3.1)\nBuilding wheels for collected packages: pydotplus\n  Building wheel for pydotplus (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/35/7b/ab/66fb7b2ac1f6df87475b09dc48e707b6e0de80a6d8444e3628\nSuccessfully built pydotplus\nInstalling collected packages: pydotplus\nSuccessfully installed pydotplus-2.0.2\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import tree\nimport pydotplus\nfrom IPython.display import Image \n\n# list of features we want to consider\nsplitting_features = [x for x in X_train.columns if x not in ['id']]\n\n# initializing the tree model and training it\ntree_model = tree.DecisionTreeClassifier()\ntree_model = tree_model.fit(X_train, y_train)\n\n# generating a graphic for the tree\ndot_data = tree.export_graphviz(tree_model, out_file=None, \n                         feature_names=splitting_features,\n                         class_names = ['Benign', 'Malignant'],\n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = pydotplus.graph_from_dot_data(dot_data)  \nImage(graph.create_png())",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "ValueError",
                    "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-6-84c3014e7167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# initializing the tree model and training it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtree_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtree_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# generating a graphic for the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2.2 Interpreting a tree graphic<a class=\"anchor\" id=\"bullet-10\"></a>\nEach rectangle represents a **node** of the tree.\n* The first line in each node identifies the splitting feature and decision being made upon it.\n* The observations are separated based on this decision into **children** nodes (left and right).\n* **Samples**: how many observations have been filtered by that decision.\n* **Value**: where how many observations are being split into the left and right children respectively.\n* **Gini**: a measurement of incorrect classification. As the algorithm learns, the gini inequality decreases to 0.\n* **Class**: the target that the observations have been grouped into: benign or malignant.\n* The rectangles at the bottom of the diagram are called **leaves**, which contain the final predictions (observations classified by diagnosis).\n\n### 2.3 How a decision tree learns<a class=\"anchor\" id=\"bullet-11\"></a>\n\nApplying the general learning outline ([section 1.3](#bullet-4)) to a decision tree:\n\n*1. Initialize the algorithm to produce a default model, and give it the feature data. The algorithm makes its first set of predictions (close to a random guess).*<br/>\nThe default model is the very first splitting feature - the algorithm chose 'concave points mean <= 0.0492'. The observations are divided into benign or malignant based on this decision, making up the first set of predictions!\n\n*2. The algorithm measures the error between its previous prediction and the true value of the targets.*<br/> \nThis error is quantified by the gini value that appears in the first node of the tree.\n\n*3. The algorithm adjusts its model-building to make the error smaller. Each adjustment represents a part of the pattern it is learning, and is used to process future data.*<br/>\nThe adjustment is the next splitting feature that is chosen for each subgroup that resulted from the concave points mean decision. The algorithm chose 'radius worst' for observations that had a concave points mean less than the threshold, and 'concavity worst' for observations with concave points mean greater than the threshold.\n\n*4. It continues to predict, calculate error, and adjust.*<br/>\nThe next set of predictions are the classifications that result from the second round of decision making. Each time, a gini value is calculated to indicate how far the classifications are from the true targets, and another set of splitting features are chosen to further refine the class groupings. \n\n### 2.4 Evaluating the model<a class=\"anchor\" id=\"bullet-12\"></a>\nPick a metric to measure predictive accuracy or performance of your model, so that you can compare different models. \n\nModels can differ in two ways:\n\n1. The algorithm used for learning (for example, the difference between a decision tree and gradient boosting)\n2. The **parameter** selection within a single algorithm (for example, a model with a learning rate set to 0.1 or 0.0)\n\nThis case refers to (1), you want to know how the tree model performs, so that it can be compared to the gradient boosting model in the next section.\n\nThe choice of evaluation metric depends on the type of learning problem. For binary classification, you can use a metric called the <a href=\"https://www.quora.com/Machine-Learning-What-is-an-intuitive-explanation-of-AUC\" target=\"_blank\" rel=\"noopener noreferrer\">AUC score</a>, which measures the probability that the model correctly identifies *malignant tumors only*. This metric is used as an indication of performance because misclassifying a malignant tumor as benign is the worst prediction scenario (compared to correct classification of benign/malignant and misclassification of benign).\n\nYou also calculate the general accuracy of the model - the percentage of observations that have been correctly classified overall. The following code uses your simple tree model to predict on the testing set, and evaluates the accuracy and AUC score of its predictions:"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import metrics\n\n#Predict test set:\ntest_predictions = tree_model.predict(X_test[splitting_features])\ntest_predprob = tree_model.predict_proba(X_test[splitting_features])[:,1]\n        \n#Print model report:\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test['diagnosis'].values, test_predictions))\nprint(\"AUC Score: %f\" % metrics.roc_auc_score(y_test['diagnosis'], test_predprob))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The model gives ~94% accuracy for all cases, and a ~94% probability of catching malignant cases in the data. Each time the code is run, the algorithm may learn slightly differently (that is, select different splitting features), which results in slight variations in accuracy and AUC scores. In the following section, you work on improving the model by stacking together many trees, and tuning parameters for better generalization ability."
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "## 3.0 Ensemble model: gradient tree boosting<a class=\"anchor\" id=\"bullet-13\"></a>\n\nStacking, or meta-ensembling, is a method of joining multiple predictive models so that the strengths of each can cover the weaknesses of others. Gradient boosting is one technique used to stack weak learners together in an **ensemble** to achieve better predictions with each additional model.\n\n3.1 [An intuitive explanation](#bullet-14)<br/>\n    3.2 [XGBoost](#bullet-15)<br/>\n    3.3 [K-fold cross validation](#bullet-16)<br/>\n\n### 3.1 An intuitive explanation of gradient boosting<a class=\"anchor\" id=\"bullet-14\"></a>\n\n1. A tree is used to learn from the data as described in [section 2.2](#bullet-10). This first model is called the **base learner**.\n2. The algorithm calculates the error of the base learner's final predictions. These error measurements are called **pseudo-residuals**. \n3. A second decision tree, called the **booster**, is stacked on top of the base tree by using the pseudo-residuals as input. Its output is a prediction of error for each prediction of the base tree. \n4. The algorithm adjusts the base prediction by adding the predicted amount of error.\n*Note: target - prediction = error, so by adding 'predicted error' to the base prediction, you are getting closer to the target.*\n5. Another set of pseudo-residuals are calculated for the predictions of the updated model (base tree + first boosting tree). Each round of boosting corrects the base prediction by a predicted amount of error, gradually inching towards the true value.\n\n\n### 3.2 XGBoost<a class=\"anchor\" id=\"bullet-15\"></a>\n\nYou use the XGBoost library, an implementation of gradient tree boosting popularized by usage in machine learning competitions. It is a valuable addition to any machine learning toolkit, as it significantly outperforms other algorithms in speed, accuracy, and flexibility. \n\nIf you don't have xgboost library installed, install it."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Note:** The xgboost package uses an older version of sklearn. When you run import xgboost, ignore the DeprecationWarning. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 3.3 K-fold cross validation<a class=\"anchor\" id=\"bullet-16\"></a>\n\nCross-validation is used to measure how well a model generalizes using the training set (no need to bring in the testing set yet!). The k-fold method divides the training data into even smaller train/test sets to gauge how the model performs on 'unseen' data.\n\n1. The data is divided into k subsamples. You use 5 subsamples: you can call them A, B, C, D, E.\n2. The model is trained on 4 subsamples (for example, A, B, C, D).\n3. The model makes predictions on the subsample that was held out from training (E), and you score it based on an evaluation metric.\n4. The model is then trained on another combination of subsamples (for example, B, C, D, E), and tested on the one that was held out (A).\n5. This repeats for every possible combination of k-folds. You take the average of the evaluation metrics at each iteration to give an idea of how the model performs on actual unseen data.\n\nYou use cross-validation during parameter tuning so that you can directly observe the effect of a parameter on a model's generalization ability."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import cross_validation",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 4.0 XGBoost: Parameter tuning<a class=\"anchor\" id=\"bullet-17\"></a><a href=\"#footnote-1\"><sup>[1]</sup></a>\n\n4.1 [Default model](#bullet-18)<br/>\n    4.2 [Tuning number of estimators](#bullet-19)<br/>\n    4.3 [Evaluating default model](#bullet-20)<br/>\n    4.4 [Interpreting evaluation](#bullet-21)<br/>\n    4.5 [Grid search](#bullet-22)<br/>\n    4.6 [Tuning tree depth and min child weight](#bullet-23)<br/>\n    4.7 [Tuning gamma](#bullet-24)<br/>\n    4.8 [Evaluating updated model](#bullet-25)<br/>\n    4.9 [Tuning sampling parameters](#bullet-26)<br/>\n    4.10 [Tuning lambda & alpha](#bullet-27)<br/>\n    4.11 [Evaluating final model](#bullet-28)<br/>\n    4.12 [Predict on the test set](#bullet-29)<br/>\n\n### 4.1 Setting up a default model<a class=\"anchor\" id=\"bullet-18\"></a>\nYou need to initialize a model with some default parameters as a starting point. The selection here depends on the nature of your data and your experience. Recall that parameters tell the algorithm how to learn in one of two ways: how complex the model will be, and how quickly it should conclude on a pattern in the data.\n\nLuckily, XGBoost performs well with its default values, so you only need to define the following three parameters: \n\n1. objective: Depends on the target type - use binary:logistic because you have a binary classification problem.\n2. learning rate: This parameter is always chosen first, as the optimal values of all other parameters are dependent upon it. It directly controls how conservative the model is in picking up patterns - a smaller value makes the model more conservative. A good rule of thumb is to initialize the learning rate at 0.1.  \n3. number of estimators: The maximum number of boosting trees to be added. Pick a number that is a bit larger than necessary for our data set size. \n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# initializing our first model with an objective and learning rate\nxgb0 = XGBClassifier(\n objective= 'binary:logistic',\n learning_rate = 0.1, \n n_estimators = 30)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.2 Tuning number of estimators<a class=\"anchor\" id=\"bullet-19\"></a>\n\nThe 'number of estimators' parameter in XGBoost refers to the maximum number of boosting trees (or rounds) to be used in building the model. Intuitively, the more trees that are added to the model, the more complex it is. This parameter is tuned first, so you can broadly adjust the complexity of your model before making adjustments that have smaller impacts on conservative learning.\n\nThe function below performs the following actions to find the best number of boosting trees to use on your data:\n\n1. Trains an XGBoost model using features of the data.\n2. Performs k-fold cross validation on the model, using accuracy and AUC score as the evaluation metric.\n3. Returns output for each boosting round so you can see how the model is learning. You will look at the detailed output in the next section.\n4. It stops running after the cross-validation score does not improve significantly with additional boosting rounds, giving you an optimal number of estimators for the model."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import matplotlib.pylab as plt\n%matplotlib inline\n\ndef evaluate_model(alg, train, target, predictors, cv_folds=5, early_stopping_rounds=1):\n    \n    xgb_param = alg.get_xgb_params()\n    xgtrain = xgb.DMatrix(train[predictors].values, target['diagnosis'].values)\n    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n        metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n    alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(train[predictors], target['diagnosis'], eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(train[predictors])\n    dtrain_predprob = alg.predict_proba(train[predictors])[:,1]\n        \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(target['diagnosis'].values, dtrain_predictions))\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(target['diagnosis'], dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importance', color='g')\n    plt.ylabel('Feature Importance Score')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.3 Evaluating the default model<a class=\"anchor\" id=\"bullet-20\"></a>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# a list of features to be used for training the model\nfeatures = [x for x in X_train.columns if x not in ['id']]\n\n# evaluating the first model\nevaluate_model(xgb0, X_train, y_train, features)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.4 Interpreting evaluation output<a class=\"anchor\" id=\"bullet-21\"></a>\n\n* The first set of readings shows you the cross-validation scores at each boosting round. So at line [0], one tree has been fit to the data. Train-auc gives you the average AUC score for the five training sets during cross-validation, and test-auc gives you the same metric for the five testing sets during cross validation. Naturally, the test score is lower, as the model is predicting on data that was not used to train it. You can see that the test-auc catches up with more boosting rounds, as more trees are used to minimize the error.\n<br/><br/>\n* The model stops training after ten rounds, which means that the cross-validation scores are not significantly improving with more than 10 rounds. For a learning rate of 0.1, you can take the number of estimators to be 10 to save computation time.\n<br/><br/>\n* Overall, the accuracy is 0.9824, which tells you how many cases were correctly classified, both benign and malignant.\n<br/><br/>\n* Overall, the AUC score is 0.99877, which indicates that the model rarely lets malignant cases slip through its screening process.\n<br/><br/>\n* The graph displays the features ranked by their importance to the model. Feature importance refers to the number of times it is used as a splitting feature in the decision trees. This function is especially useful in large datasets, where many variables are just noise and <a href=\"https://en.wikipedia.org/wiki/Feature_engineering\" target=\"_blank\" rel=\"noopener no referrer\">feature engineering</a> may be required.\n\n### 4.5 Grid search<a class=\"anchor\" id=\"bullet-22\"></a>\nTo tune the remaining XGBoost parameters, use grid search with cross-validation.\n\n1. Define set of values for the parameter in question. These values of interest again depend on the nature of the data and your experience.\n2. The grid search function systematically produces a model for each unique combination of these values, and evaluates it using cross-validation.\n3. The optimal parameter values are found in the model with the highest cv score."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.model_selection import GridSearchCV",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.6 Tuning max depth and child weight<a class=\"anchor\" id=\"bullet-23\"></a>\n\n**max depth**: The size of each new decision tree. Smaller trees = less complexity. <br/>\n**min child weight**: The minimum number of observations that must be in the children after a split. Smaller weight = more conservative."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# updating our default model with the optimal number of estimators\nxgb1 = XGBClassifier(\n objective = 'binary:logistic',\n learning_rate =0.1,\n n_estimators=10)\n\n# array of values for max_depth and min_child_weight parameters\nparam_test1 = {'max_depth':range(3,7,1),'min_child_weight':range(1,3,1)}\n\n# grid search with cross-validation using the updated model and parameter value array \ngsearch1 = GridSearchCV(estimator = xgb1, param_grid = param_test1, scoring='roc_auc',iid=False, cv=5)\ngsearch1.fit(X_train[features],y_train['diagnosis'])\ngsearch1.cv_results_['params'], gsearch1.best_params_, gsearch1.best_score_",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The grid search found that the model works best with a max depth of 5, and a minimum child weight of 2. You can update our model accordingly and continue tuning.\n\n### 4.7 Tuning gamma<a class=\"anchor\" id=\"bullet-24\"></a>\nIncreasing gamma makes the algorithm more conservative (less prone to overfitting)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# updating our current model with the max_depth and min_child weight parameter values found in last grid search\nxgb2 = XGBClassifier(\n objective='binary:logistic',\n learning_rate =0.1,\n n_estimators=10,\n max_depth=5,\n min_child_weight=2,\n gamma=0)\n\n# array of values for the gamma parameter\ngamma_test = {'gamma':[i/100.0 for i in range(0,6)]}\n\n# grid search with cross-validation using the updated model and gamma value array \ngsearch2 = GridSearchCV(estimator = xgb2, param_grid = gamma_test, scoring='roc_auc',iid=False, cv=5)\ngsearch2.fit(X_train[features],y_train['diagnosis'])\ngsearch2.cv_results_['params'], gsearch2.best_params_, gsearch2.best_score_",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The grid search found that the model works best when gamma is 0.03."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.8 Evaluating updated model<a class=\"anchor\" id=\"bullet-25\"></a>\n\nRevisit your evaluation function to see if you need to update the number of boosting rounds. Use the parameter values found for max_depth, min_child_weight, and gamma, and set the number of estimators as 30 to see when it stops running."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "xgb_check = XGBClassifier(\n objective='binary:logistic',\n learning_rate =0.1,\n n_estimators=30,\n max_depth=5,\n min_child_weight=2,\n gamma=0.03)\n\nevaluate_model(xgb_check, X_train, y_train, features)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.9 Tuning sampling parameters<a class=\"anchor\" id=\"bullet-26\"></a>\n\n**subsample**: The ratio of data that is randomly selected for growing trees<br/>\n**colsample_bytree**: The ratio of the subsample from which the algorithm selects splitting features\n\n**Note:** You have reduced your data set several times now; originally, you divided it into 80/20 train and test sets. You input only the training set to XGBoost, which are segmented twice more by subsample and colsample. Perhaps less than 50% of the original data will be used to train the model, reducing the chance of overfitting and improving the model's ability to generalize. It is one of the reasons why you see a better test score for XGBoost compared to the single decision tree model. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# updating our current model with the most recent n_estimators\nxgb3 = XGBClassifier(\n objective='binary:logistic',\n learning_rate =0.1,\n n_estimators=23,\n max_depth=5,\n min_child_weight=2,\n gamma=0.03)\n\n# array of values for subsample and colsample_bytree parameters\nsample_test = {\n 'subsample':[i/10.0 for i in range(5,9)],\n 'colsample_bytree':[i/10.0 for i in range(5,9)]\n}\n\n# grid search with cross validation for sampling parameters\ngsearch3 = GridSearchCV(estimator = xgb3, param_grid = sample_test, scoring='roc_auc',iid=False, cv=5)\ngsearch3.fit(X_train[features],y_train['diagnosis'])\ngsearch3.cv_results_['params'], gsearch3.best_params_, gsearch3.best_score_",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The grid search found that the model works best with a colsample_bytree of 0.5, and a subsample of 0.7."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.10 Tuning lambda and alpha<a class=\"anchor\" id=\"bullet-27\"></a>\n\nLambda and alpha are both regularization parameters, which mathematically reduce the impact of features that might be too dominant in the model. The difference between the two is in how they apply these penalties.\n\n**lambda**: Applies L2 regularization, which shrinks the weights of all selected features equally. The default value is 0. <br/>\n**alpha**: Applies L1 regularization, which can shrink the weights down to 0 - essentially discarding features that have little impact on the model. The default is 1.\n\nThese parameters have more significant effects on large models (with more features). Tune them to see how it impacts your model."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# updating our current model with the sampling parameters found in the last grid search\nxgb4 = XGBClassifier(\n objective='binary:logistic',\n learning_rate =0.1,\n n_estimators=23,\n max_depth=5,\n min_child_weight=2,\n gamma=0.03, \n subsample=0.7,\n colsample_bytree=0.5)\n\n# array of values for subsample and colsample_bytree parameters\nreg_test = {'reg_alpha':[0, 0.01, 0.1], 'reg_lambda':[1, 1.1, 1.2, 1.3]}\n\n# grid search with cross validation for regularization parameters\ngsearch4 = GridSearchCV(estimator = xgb4, param_grid = reg_test, scoring='roc_auc',iid=False, cv=5)\ngsearch4.fit(X_train[features],y_train['diagnosis'])\ngsearch4.cv_results_['params'], gsearch4.best_params_, gsearch4.best_score_",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The grid search found that the model works best when the reg_alpha regualization is 0.01 and the reg_lambda regulization is 1."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.11 Evaluating final model<a class=\"anchor\" id=\"bullet-28\"></a>\nRun the current model through your evaluation function a final time. Compared to the model with default parameters, tuning has improved accuracy from 0.9824 to 0.9912, and the AUC score from 0.998772 to 0.999195. You can see why XGBoost is most popular for competition use, where those fourth decimal places really count!"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# updated model with alpha value found with last grid search. (lambda does not need to be set, since default is 1)\nxgb_final = XGBClassifier(\n objective='binary:logistic',\n learning_rate =0.1,\n n_estimators=30,\n max_depth=5,\n min_child_weight=2,\n gamma=0.03,\n reg_alpha=0.01)\n\nevaluate_model(xgb_final, X_train, y_train, features)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.12 Predict on the test set<a class=\"anchor\" id=\"bullet-29\"></a>\nFinally, predict on the test data you reserved at the beginning. The XGBoost ensemble gives you approximately a 3% increase in accuracy and a 5% increase in AUC score over a single decision tree."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "test_features = [x for x in X_test.columns if x not in ['id']]\n\n#Predict test set:\ntest_predictions = xgb_final.predict(X_test[test_features])\ntest_predprob = xgb_final.predict_proba(X_test[test_features])[:,1]\n        \n#Print model report:\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test['diagnosis'].values, test_predictions))\nprint(\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test['diagnosis'], test_predprob))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "If you want to increase the accuracy and AUC score even further, you can restart the parameter tuning process with a smaller learning rate. The next value to try is 0.01. "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Summary\nYou've found out how to use a decision tree as a machine learning model to classify cancer data, and you've seen how you how tools like XGBoost can be used to tune parameters to improve the accuracy of your model.\n\n### Learn more:\n* <p id=\"footnote-1\"><sup>[1]</sup><a id=\"fn1\" href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python\" target=\"_blank\" rel=\"noopener noreferrer\">Jain, A. (March 1, 2016) Complete Guide to Parameter Tuning in XGBoost(with codes in Python)</a>\n* <a href=\"https://en.wikipedia.org/wiki/Feature_engineering\" target=\"_blank\" rel=\"noopener no referrer\">Feature engineering</a>\n* <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\" target=\"_blank\" rel=\"noopener noreferrer\">Decision tree learning metrics</a>\n* <a href=\"https://www.quora.com/Machine-Learning-What-is-an-intuitive-explanation-of-AUC\" rel=\"noopener noreferrer\">An intuitive explanation of AUC</a>\n* <a href=\"http://www.wikihow.com/Play-20-Questions\" target=\"_blank\" rel=\"noopener noreferrer\">How to play 20 questions</a>\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}