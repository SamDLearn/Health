{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<table style='border: none' align='left'>\n   <tr style='border: none'>\n      <th style='border: none'><font face='verdana' size='5' color='black'><b>Use XGBoost to classify tumors with IBM Watson Machine Learning</b></th>\n      <th style='border: none'><img src='https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true' alt='Watson Machine Learning icon' height='40' width='40'></th>\n   </tr>\n   <tr style='border: none'>\n       <th style='border: none'><img src='https://raw.githubusercontent.com/pmservice/wml-sample-notebooks/master/images/cancer_banner-06.png' alt='Icon' width='700'> </th>\n   </tr>\n</table>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This notebook demonstates how to obtain data from the IBM Watson Studio Community, create a predictive model, and score the model.\nSome familiarity with Python is helpful. This notebook is compatible with Python 3.6 and uses XGBoost and scikit-learn.\n\nYou will use a publicly available data set, the Breast Cancer Wisconsin (Diagnostic) data set, to train an XGBoost Model to classify breast cancer tumors (as benign or malign.) There are 569 data points in the Breast Cancer Wisconsin (Diagnostic) data set, and each data point has predictors such as radius, texture, perimeter, and area. XGBoost stands for \u201cE**x**treme **G**radient **Boost**ing\u201d.\n\nThe XGBoost classifier makes its predictions based on the majority vote from a collection of models which are a set of classification trees. It uses the combination of weak learners to create a single strong learner. It is a sequential training process where new learners focus on the misclassified examples of previous learners.\n\n\n## Learning goals\n\nYou will learn how to:\n\n-  Load a CSV file into a pandas dataframe\n-  Explore data\n-  Prepare data for training and evaluation\n-  Create an XGBoost machine learning model\n-  Train and evaluate the model\n-  Use cross-validation to optimize model's hyperparameters\n-  Persist the model in the Watson Machine Learning repository\n-  Deploy the model for online scoring\n-  Score test data\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1.\t[Set up the environment](#setup)\n2.\t[Load and explore the data](#load)\n3.\t[Create an XGBoost model](#model)\n4.\t[Persist the model](#persistence)\n5.\t[Deploy and score the model in the WML repository](#scoring)\n6.\t[Summary and next steps](#summary)"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='setup'></a>\n## 1. Set up the environment\n\nBefore running the code in this notebook, please make sure you have the following requirements:\n\n-  A <a href=\"https://cloud.ibm.com/catalog/services/machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create an instance can be found <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)\n\n-  Local python environment configurations:\n  + Python 3.6\n  + xgboost\n  + watson-machine-learning-client\n  + pixiedust\n  + matplotlib\n  + seaborn\n\n- Download **Breast Cancer Wisconsin (Diagnostic) Data Set** dataset from the Watson Studio <a href=\"https://dataplatform.ibm.com/community?context=analytics\" target=\"_blank\" rel=\"noopener no referrer\">Community</a>.\n\n**Note:** We provide the code to download the data set in [step 2](#load)."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='load'></a>\n## 2. Load and explore the data"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this section, you will load the data into a pandas dataframe and perform an exploratory data analysis (EDA).\n\nTo load the data into a pandas dataframe, use `wget` to download the data first and `pandas` to read the data."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Example**: First, you need to install the required packages. You can do this by running the following code. Run it only once.<BR><BR>"
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "!pip install --upgrade wget",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Get the data.\nimport wget\nimport os",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The csv file **BreastCancerWisconsinDataSet.csv** is downloaded. Run the code in the following cells to load the file as a pandas dataframe."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Note:** Update the packages to ensure you have the latest version."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "`pixiedust` is an open-source Python helper library that works as an add-on to Jupyter notebooks to improve the user experience of working with data.  \n`pixiedust` documentation/code can be found <a href=\"https://github.com/pixiedust/pixiedust\" target=\"_blank\" rel=\"noopener no referrer\">here</a>.  "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_ec95c633090441099a8c5ecc71550224 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='hrdnVScSXm_Ub_Fy8XiSU6EDld79-0fIfEGzX2zEmvjQ',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_ec95c633090441099a8c5ecc71550224.get_object(Bucket='health-donotdelete-pr-retyx20ewuwekd',Key='BreastCancerwisconsin_data.csv.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# If you are reading an Excel file into a pandas DataFrame, replace `read_csv` by `read_excel` in the next statement.\ndf = pd.read_csv(body)\ndf.head()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade pixiedust",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Import ``pixiedust``."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pixiedust",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can run the following method if you don't want pixiedust collecting user statistics."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pixiedust.optOut()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this notebook, ``pixiedust`` will only be used as a dataframe viewer. However, ``pixiedust`` can also be used as a data visualization tool. You can find the details of the visualization functionality of ``pixiedust`` <a href=\"https://pixiedust.github.io/pixiedust/displayapi.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."
        },
        {
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "tableView"
                    }
                }
            },
            "cell_type": "code",
            "source": "display(df)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Run the code in the next cell to view the predictor names and data types.\n\nYou can see that the data set has 569 data points and 31 predictors."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Information about the data set, predictor names, and data types.\ndf.info()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Information about values in the numerical columns.\ndf.describe()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can see the distribution of the target values/labels by running the following code."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Distribution of target values/labels.\ndf['diagnosis'].value_counts()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check for NANs.\ndf.isnull().sum().sum()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The data set has no missing values."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In order to make accurate predictions, you need to select the significant predictors by choosing the features that most affect the output, *diagnosis*."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# pairwise correlation of numerical columns\ndf.corr()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "# Import seaborn and matplotlib for data exploration/visialization.\n!pip install --upgrade seaborn\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "#plot a correlation heatmap\nplt.subplots(figsize=(25,20))\nhm1 = sns.heatmap(df.corr(), annot=True, cmap='YlGnBu')\nhm1.set_xticklabels(hm1.get_xticklabels(), rotation=90)\nhm1.xaxis.set_ticks_position('top')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This correlation heatmap helps with feature selection because the gradient shows the correlation between the columns of the dataframe. In order to select only the *significant* predictors, you must eliminate features that are highly correlated with each other **(ex: 0.95)**."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "With respect to predicting the labels, the most significant predictors can be found by plotting boxplots of the numerical values against the labels. The features with boxplots that show the most variance should be chosen as the predictors for your model."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# plot boxplots of numerical columns\ncont_list = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'symmetry_mean', 'fractal_dimension_mean']\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(20, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n\nfor i in range(len(cont_list)):\n    sns.boxplot(x = 'diagnosis', y = cont_list[i], data=df, ax=ax[i])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot boxplots of numerical columns.\ncont_list2 = ['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'symmetry_se', 'fractal_dimension_se']\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(20, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n\nfor i in range(len(cont_list2)):\n    sns.boxplot(x = 'diagnosis', y = cont_list2[i], data=df, ax=ax[i])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot boxplots of numerical columns.\ncont_list3 = ['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst']\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(20, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n\nfor i in range(len(cont_list3)):\n    sns.boxplot(x = 'diagnosis', y = cont_list3[i], data=df, ax=ax[i])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Here are boxplots of the most significant features:"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# Compare boxplots of significant numerical columns.\ncont_list4 = ['radius_mean', 'radius_se', 'radius_worst', 'area_mean', 'area_se', 'area_worst', 'compactness_mean', 'compactness_se', 'compactness_worst', 'concavity_mean', 'concavity_se', 'concavity_worst']\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9), (ax10, ax11, ax12)) = plt.subplots(4, 3, figsize=(20, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n\nfor i in range(len(cont_list4)):\n    sns.boxplot(x = 'diagnosis', y = cont_list4[i], data=df, ax=ax[i])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "cont_list_f = ['radius_mean', 'concavity_mean', 'texture_mean', 'area_worst', 'compactness_worst']\nplt.subplots(figsize=(15,10))\nhm2 = sns.heatmap(df[cont_list_f].corr(), annot=True, cmap='YlGnBu')\nhm2.set_xticklabels(hm2.get_xticklabels(), rotation=90)\nhm2.xaxis.set_ticks_position('top')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "By plotting the boxplots of each numerical column against the diagnosis type, we have picked out the significant features/predictors. More variation in the boxplot implies higher significance. We also eliminate features that are highly correlated. Therefore we can choose *radius_mean, radius_se, compactness_worst, concavity_mean, texture_mean* as the predictors for our model."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='model'></a>\n## 3. Create an XGBoost model\n\nIn this section, you will learn how to train and test an XGBoost model.\n\n- [3.1 Split data](#prepare)\n- [3.2 Create an XGBoost model](#create)"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 3.1 Split data<a id='prepare'></a>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You will pass the data with the selected significant predictors to build the model. You will use the `diagnosis` column as your target variable."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Choosing the significant predictors.\n\nX = df.iloc[:, [1,2,7,24,26]]\nX = X.values\n\n# Changing the target variables to binary variables\ny = (df['diagnosis'] == 'M').astype(int)\ny = y.values",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Split the data set into: \n- Train data set\n- Test data set"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install scikit-learn==0.19.1",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Split the data set and create two data sets.\nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=143)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# List the number of records in each data set.\nprint('Number of training records: ' + str(len(X_train)))\nprint('Number of testing records : ' + str(len(X_test)))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The data has been successfully split into two data sets:\n- The train data set which is the largest group will be used for training.\n- The test data set will be used for model evaluation and is used to test the assumptions of the model."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 3.2 Create an XGBoost model<a id='create'></a>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Install required packages.\n\n**Tip:** Make sure `xgboost`'s version is 0.80."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install 'xgboost==0.80'",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import xgboost\nxgboost.__version__",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Import packages you need to create the XGBoost model.\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 3.2.1 Create an XGBoost classifier"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this subsection, you will create an XGBoost classifier with default hyperparameters and you will call *xgb_model*. \n\n**Note**: The next sections show you how to improve this base model."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create the XGB classifier - xgb_model.\nxgb_model = XGBClassifier(n_estimators=100)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Display the default parameters for *xgb_model*."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# List the default parameters.\nprint(xgb_model.get_xgb_params())",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now, that your XGBoost classifier *xgb_model* is set up, you can train it by using the fit method. You will also evaluate *xgb_model* as the train and test data are being trained."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# Train and evaluate.\nxgb_model.fit(X_train, y_train, eval_metric=['error'], eval_set=[((X_train, y_train)),(X_test, y_test)])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Plot the model performance evaluated during the training process to assess model overfitting."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot and display the performance evaluation\nxgb_eval = xgb_model.evals_result()\neval_steps = range(len(xgb_eval['validation_0']['error']))\n\nfig, ax = plt.subplots(1, 1, sharex=True, figsize=(8, 6))\n\nax.plot(eval_steps, [1-x for x in xgb_eval['validation_0']['error']], label='Train')\nax.plot(eval_steps, [1-x for x in xgb_eval['validation_1']['error']], label='Test')\nax.legend()\nax.set_title('Accuracy')\nax.set_xlabel('Number of iterations')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can see that there is model overfitting, and there is no increase in model accuracy after about 40 iterations.\n\nSelect the trained model obtained after 40 iterations."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Select trained model.\nn_trees = 40\ny_pred = xgb_model.predict(X_test, ntree_limit= n_trees)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check the accuracy of the trained model.\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy: %.2f%%' % (accuracy * 100.0))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Note:** You will use the test data accuracy to compare the accuracy of the model with *default* parameters to the accuracy of the model with *tuned* parameters."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nplt.matshow(cm)\nplt.colorbar()\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\nplt.show()\npd.DataFrame(cm)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This confusion matrix maps the predicted values against the actual values. Here, you can see that 126 benign tumors and 66 malignant tumors have been predicted correctly. However, 8 benign tumors have been incorrectly predicted as malignant. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred_prob = xgb_model.predict_proba(X_test)\n\n# ROC-AUC curve\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, 1])\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This is the ROC-AUC curve - the area under the curve represents the accuracy of the predictions. You can see that the area under the curve is large, indicating that the predictions are highly accurate."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 3.2.2 Use grid search and cross-validation to tune the model "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can use grid search and cross-validation to tune your model to achieve better accuracy.\n\n**Note**: Grid search is used for this model as an example, but it is **not** recommended for small data sets such as this one, as it might lead to overfitting.\n\nXGBoost has an extensive catalog of hyperparameters which provides great flexibility to shape an algorithm\u2019s desired behavior. Here you will the optimize the model tuning which adds an L1 penalty (`reg_alpha`).\n\nUse a 5-fold cross-validation because your training data set is small."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In the cell below, create the XGBoost pipeline and set up the parameter grid for the grid search."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create XGBoost pipeline, set up parameter grid.\nxgb_model_gs = XGBClassifier()\nparameters = {'reg_alpha': [0.0, 1.0, 2.0], 'reg_lambda': [0.0, 1.0, 2.0], 'n_estimators': [n_trees], 'seed': [1337]}",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Use ``GridSearchCV`` to search for the best parameters from the specified values in the previous cell."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# Search for the best parameters.\nclf = GridSearchCV(estimator = xgb_model_gs, param_grid = parameters, scoring='accuracy', cv=5, verbose=1, n_jobs=1, refit=True)\nclf.fit(X_train, y_train)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can see the cross validation results that were evaluated by the grid search."
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "# Print model cross validation results.\nfor key in ['params', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']:\n    print(str(key) + ': \\n' + str(clf.cv_results_[key]) + '\\n\\n')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Display the accuracy estimated using cross-validation and the hyperparameter values for the best model."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print('Best score: %.1f%%' % (clf.best_score_*100))\nprint('Best parameter set: %s' % (clf.best_params_))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Display the accuracy of the best parameter combination on the test set."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred = clf.best_estimator_.predict(X_test, ntree_limit= n_trees)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %.2f%%' % (accuracy * 100.0))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The test set's accuracy is about the same for both the tuned model and the trained model with default hyperparameter values, even though the tuned hyperparameters are different from the default parameters."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 3.2.3 Model with pipeline data preprocessing"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this subsection, you will learn how to use the XGBoost model within the scikit-learn pipeline. \n\nLet's start by importing the required modules."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pca = PCA(n_components=5)\nxgb_model_pca = XGBClassifier(n_estimators=n_trees)\npipeline = Pipeline(steps=[('pca', pca), ('xgb', xgb_model_pca)])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pipeline.fit(X_train, y_train)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now, you are ready to evaluate the accuracy of the model trained on the reduced set of features."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred = pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %.2f%%' % (accuracy * 100.0))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can see that this model has an accuracy similar to the model trained using default hyperparameters."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's see how you can save the XGBoost pipeline using the WML service instance and deploy it for online scoring."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='persistence'></a>\n## 4. Persist the model"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this section, you will learn how to use the `watson-machine-learning-client` package to store your XGBoost model in the WML repository."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!rm -rf $PIP_BUILD/watson-machine-learning-client",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade watson-machine-learning-client",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Authenticate the Watson Machine Learning service on the IBM Cloud.\n\n**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-get-wml-credentials.html\" target=\"_blank\" rel=\"noopener no referrer\">Service Credentials</a> tab of the service instance that you created on the IBM Cloud. <BR>If you cannot find the **instance_id** field in **Service Credentials**, click **New credential (+)** to generate new authentication information. \n\n**Action**: Enter your Watson Machine Learning service instance credentials here."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "wml_credentials = {\n    \"apikey\": \"***\",\n    \"instance_id\": \"***\",\n    \"password\": \"***\",\n    \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n    \"username\": \"***\"\n}",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "client = WatsonMachineLearningAPIClient(wml_credentials)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 4.1 Save the XGBoost model in the WML Repository\n\nSave the model artifact as *XGBoost model for breast cancer* to your WML instance."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model_props = {client.repository.ModelMetaNames.NAME: 'XGBoost model for breast cancer'}\nmodel_details = client.repository.store_model(pipeline, model_props)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(model_details)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Get the saved model metadata from WML."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# 5. Deploy and score in the WML repository <a id=\"scoring\"></a>\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this section, you will learn how to create online scoring and score a new data record in the WML repository."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can list all stored models using the  `list_models` method."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# Display a list of all the models.\nclient.repository.list_models()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You need the model uid to create the deployment. You can extract the model uid from the saved model details."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Extract the uid.\nmodel_uid = client.repository.get_model_uid(model_details)\nprint(model_uid)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Use this modul_uid in the next section to create the deployment."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 5.1 Create a model deployment"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now, you can create a deployment, *Predict breast cancer*."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create the deployment.\ndeployment_details = client.deployments.create(model_uid, 'Predict breast cancer')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Get the list of all deployments."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# List the deployments.\nclient.deployments.list()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The *Predict breast cancer* model has been successfully deployed."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 5.2 Perform prediction"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now, extract the url endpoint, *scoring_url*, which will be used to send scoring requests."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Extract endpoint url and display it.\nscoring_url = client.deployments.get_scoring_url(deployment_details)\nprint(scoring_url)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Prepare the scoring payload with the values to score."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Prepare scoring payload.\npayload_scoring = {'values': [list(X_test[0]), list(X_test[1])]}\nprint(payload_scoring)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "# Perform prediction and display the result.\nimport json\nresponse_scoring = client.deployments.score(scoring_url, payload_scoring)\nprint(json.dumps(response_scoring, indent=2))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Result**: The patient records are classified as a benign tumor and a malignant tumor respectively."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='summary'></a>\n## 6. Summary and next steps     "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You have successfully completed this notebook! \n\nYou learned how to use a machine learning algorithm called XGBoost as well as Watson Machine Learning to create and deploy a model. \n\nCheck out our <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a> for more samples, tutorials, documentation, how-tos, and blog posts. "
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}